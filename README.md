# Entity & Quantity Correction for Abstractive Summarization

This repository contains the code and resources to ["Improving Faithfulness in Abstractive Summarization
with Contrast Candidate Generation and Selection"](https://www.seas.upenn.edu/~sihaoc/static/pdf/CZSR21.pdf) in NAACL'21.  
```
@inproceedings{CZSR21,
    author = {Sihao Chen and Fan Zhang and Kazoo Sone and Dan Roth},
    title = {{Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection}},
    booktitle = {NAACL},
    year = {2021}
}
```

## Use the Correction Model for Summary Ranking
The trained BART-base model for classifying whether a summary is hallucinated/faithful is published to huggingface model hub as [`CogComp/bart-faithful-summary-detector`](https://huggingface.co/CogComp/bart-faithful-summary-detector). With the `transformers` library installed, you can use it as follows.  

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("CogComp/bart-faithful-summary-detector")
model = AutoModelForSequenceClassification.from_pretrained("CogComp/bart-faithful-summary-detector")

article = "Ban-Ki Moon was re-elected for a second term by the UN General Assembly, unopposed and unanimously, on 21 June 2011"

bad_summary = "Ban Ki-moon was elected for a second term in 2007"
good_summary = "Ban Ki-moon was elected for a second term in 2011"

bad_pair = tokenizer(text=bad_summary, text_pair=article, return_tensors='pt')
good_pair = tokenizer(text=good_summary, text_pair=article, return_tensors='pt')

bad_score = model(**bad_pair)
good_score = model(**good_pair)

print(good_score[0][:, 1] > bad_score[0][:, 1]) # True, label mapping: "0" -> "Hallucinated" "1" -> "Faithful"
```

## Example Corrections and Evaluation 
We include the 1,510 examples in XSum test set that our method made corrections to under `data/`. 

- `source.part.txt`: source text/article
- `target.part.txt`: ground truth summary
- `bart.part.txt`: summaries generated by the [BART (large)](https://huggingface.co/facebook/bart-large-xsum) baseline
- `corrected.part.txt`: summaries corrected by our system 

To reproduce the evaluation results:

- `ROUGE`: We use version `0.0.4` of [`rouge-score`](https://pypi.org/project/rouge-score/) library. 
- `BertScore`: We use version `0.3.6` of [bert-score](https://github.com/Tiiiger/bert_score), with the `roberta-large_L17_no-idf_version=0.3.6` model. See their [github readme]((https://github.com/Tiiiger/bert_score)) for instructions. 
- `FEQA`: See the example usage below for `run_feqa.py`. Check the [FEQA](https://github.com/esdurmus/feqa/blob/master/feqa.py) repo for the complete list of required libraries. Note: You may want to use a fresh environment for FEQA, as it requires a different version of `transformers`. 

```bash
python run_feqa.py \  
    --source_file data/source.part.txt
    --summary_file data/corrected.part.txt
    --result_file data/feqa_corrected_results.json
```
## Create Unfaithful Variants via Entity Perturbation
The script for this will be updated in the next few days.

## Training 
The training data and validation data we generated can be downloaded from this [google drive folder](https://drive.google.com/drive/folders/18Eqfemxf6wOQeSUNrZMlacF2OvwaRQ87?usp=sharing).

The training script will be updated in the next few days.  
